{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from gaussian_splatting.utils.graphics_utils import getProjectionMatrix2, getWorld2View2\n",
    "from utils.slam_utils import image_gradient, image_gradient_mask\n",
    "\n",
    "\n",
    "class Camera(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        uid,\n",
    "        color,\n",
    "        depth,\n",
    "        gt_T,\n",
    "        projection_matrix,\n",
    "        fx,\n",
    "        fy,\n",
    "        cx,\n",
    "        cy,\n",
    "        fovx,\n",
    "        fovy,\n",
    "        image_height,\n",
    "        image_width,\n",
    "        device=\"cuda:0\",\n",
    "    ):\n",
    "        super(Camera, self).__init__()\n",
    "        self.uid = uid\n",
    "        self.device = device\n",
    "\n",
    "        T = torch.eye(4, device=device)\n",
    "        self.R = T[:3, :3]\n",
    "        self.T = T[:3, 3]\n",
    "        self.R_gt = gt_T[:3, :3]\n",
    "        self.T_gt = gt_T[:3, 3]\n",
    "\n",
    "        self.original_image = color\n",
    "        self.depth = depth\n",
    "        self.grad_mask = None\n",
    "\n",
    "        self.fx = fx\n",
    "        self.fy = fy\n",
    "        self.cx = cx\n",
    "        self.cy = cy\n",
    "        self.FoVx = fovx\n",
    "        self.FoVy = fovy\n",
    "        self.image_height = image_height\n",
    "        self.image_width = image_width\n",
    "\n",
    "        self.rgb_pyramid = None\n",
    "        self.depth_pyramid = None \n",
    "        self.intrinsics_pyramid = None \n",
    "\n",
    "        self.cam_rot_delta = nn.Parameter(\n",
    "            torch.zeros(3, requires_grad=True, device=device)\n",
    "        )\n",
    "        self.cam_trans_delta = nn.Parameter(\n",
    "            torch.zeros(3, requires_grad=True, device=device)\n",
    "        )\n",
    "\n",
    "        self.exposure_a = nn.Parameter(\n",
    "            torch.tensor([0.0], requires_grad=True, device=device)\n",
    "        )\n",
    "        self.exposure_b = nn.Parameter(\n",
    "            torch.tensor([0.0], requires_grad=True, device=device)\n",
    "        )\n",
    "\n",
    "        self.projection_matrix = projection_matrix.to(device=device)\n",
    "\n",
    "    @staticmethod\n",
    "    def init_from_dataset(dataset, idx, projection_matrix):\n",
    "        gt_color, gt_depth, gt_pose = dataset[idx]\n",
    "        return Camera(\n",
    "            idx,\n",
    "            gt_color,\n",
    "            gt_depth,\n",
    "            gt_pose,\n",
    "            projection_matrix,\n",
    "            dataset.fx,\n",
    "            dataset.fy,\n",
    "            dataset.cx,\n",
    "            dataset.cy,\n",
    "            dataset.fovx,\n",
    "            dataset.fovy,\n",
    "            dataset.height,\n",
    "            dataset.width,\n",
    "            device=dataset.device,\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def init_from_gui(uid, T, FoVx, FoVy, fx, fy, cx, cy, H, W):\n",
    "        projection_matrix = getProjectionMatrix2(\n",
    "            znear=0.01, zfar=100.0, fx=fx, fy=fy, cx=cx, cy=cy, W=W, H=H\n",
    "        ).transpose(0, 1)\n",
    "        return Camera(\n",
    "            uid, None, None, T, projection_matrix, fx, fy, cx, cy, FoVx, FoVy, H, W\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def world_view_transform(self):\n",
    "        return getWorld2View2(self.R, self.T).transpose(0, 1)\n",
    "\n",
    "    @property\n",
    "    def full_proj_transform(self):\n",
    "        return (\n",
    "            self.world_view_transform.unsqueeze(0).bmm(\n",
    "                self.projection_matrix.unsqueeze(0)\n",
    "            )\n",
    "        ).squeeze(0)\n",
    "\n",
    "    @property\n",
    "    def camera_center(self):\n",
    "        return self.world_view_transform.inverse()[3, :3]\n",
    "\n",
    "    def update_RT(self, R, t):\n",
    "        self.R = R.to(device=self.device)\n",
    "        self.T = t.to(device=self.device)\n",
    "\n",
    "    def compute_grad_mask(self, config):\n",
    "        edge_threshold = config[\"Training\"][\"edge_threshold\"]\n",
    "\n",
    "        gray_img = self.original_image.mean(dim=0, keepdim=True)\n",
    "        gray_grad_v, gray_grad_h = image_gradient(gray_img)\n",
    "        mask_v, mask_h = image_gradient_mask(gray_img)\n",
    "        gray_grad_v = gray_grad_v * mask_v\n",
    "        gray_grad_h = gray_grad_h * mask_h\n",
    "        img_grad_intensity = torch.sqrt(gray_grad_v**2 + gray_grad_h**2)\n",
    "\n",
    "        if config[\"Dataset\"][\"type\"] == \"replica\":\n",
    "            row, col = 32, 32\n",
    "            multiplier = edge_threshold\n",
    "            _, h, w = self.original_image.shape\n",
    "            for r in range(row):\n",
    "                for c in range(col):\n",
    "                    block = img_grad_intensity[\n",
    "                        :,\n",
    "                        r * int(h / row) : (r + 1) * int(h / row),\n",
    "                        c * int(w / col) : (c + 1) * int(w / col),\n",
    "                    ]\n",
    "                    th_median = block.median()\n",
    "                    block[block > (th_median * multiplier)] = 1\n",
    "                    block[block <= (th_median * multiplier)] = 0\n",
    "            self.grad_mask = img_grad_intensity\n",
    "        else:\n",
    "            median_img_grad_intensity = img_grad_intensity.median()\n",
    "            self.grad_mask = (\n",
    "                img_grad_intensity > median_img_grad_intensity * edge_threshold\n",
    "            )\n",
    "\n",
    "    def get_pyramid(n_levels):\n",
    "        \"\"\" function to build the levels of the pyramid for tracking\n",
    "        updates the rgb and the image and, the lists are sorted from \n",
    "        coarse to fine lebels\"\"\"\n",
    "        rgb_pyramid = []\n",
    "        depth_pyramid = []\n",
    "\n",
    "        for i in reversed(range(n_levels)):\n",
    "            pass \n",
    "\n",
    "    def clean(self):\n",
    "        self.original_image = None\n",
    "        self.depth = None\n",
    "        self.grad_mask = None\n",
    "\n",
    "        self.cam_rot_delta = None\n",
    "        self.cam_trans_delta = None\n",
    "\n",
    "        self.exposure_a = None\n",
    "        self.exposure_b = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "from utils.pose_utils import update_pose\n",
    "from utils.slam_utils import get_loss_tracking, get_median_depth\n",
    "from gaussian_splatting.gaussian_renderer import render\n",
    "\n",
    "class FrontEnd:\n",
    "    def __init__(self, config) -> None:\n",
    "        self.use_every_f_frames = 1\n",
    "        self.config = config \n",
    "        self.n_levels = config['Training']['n_pyramid_levels']\n",
    "\n",
    "    def tracking(self, cur_frame_idx, viewpoint):\n",
    "            prev = self.cameras[cur_frame_idx - self.use_every_n_frames]\n",
    "            viewpoint.update_RT(prev.R, prev.T)\n",
    "            viewpoint.get_pyramid(self.n_levels)        \n",
    "\n",
    "            opt_params = []\n",
    "            opt_params.append(\n",
    "                {\n",
    "                    \"params\": [viewpoint.cam_rot_delta],\n",
    "                    \"lr\": self.config[\"Training\"][\"lr\"][\"cam_rot_delta\"],\n",
    "                    \"name\": \"rot_{}\".format(viewpoint.uid),\n",
    "                }\n",
    "            )\n",
    "            opt_params.append(\n",
    "                {\n",
    "                    \"params\": [viewpoint.cam_trans_delta],\n",
    "                    \"lr\": self.config[\"Training\"][\"lr\"][\"cam_trans_delta\"],\n",
    "                    \"name\": \"trans_{}\".format(viewpoint.uid),\n",
    "                }\n",
    "            )\n",
    "            opt_params.append(\n",
    "                {\n",
    "                    \"params\": [viewpoint.exposure_a],\n",
    "                    \"lr\": 0.01,\n",
    "                    \"name\": \"exposure_a_{}\".format(viewpoint.uid),\n",
    "                }\n",
    "            )\n",
    "            opt_params.append(\n",
    "                {\n",
    "                    \"params\": [viewpoint.exposure_b],\n",
    "                    \"lr\": 0.01,\n",
    "                    \"name\": \"exposure_b_{}\".format(viewpoint.uid),\n",
    "                }\n",
    "            )\n",
    "\n",
    "            pose_optimizer = torch.optim.Adam(opt_params)\n",
    "            for tracking_itr in range(self.tracking_itr_num):\n",
    "                if tracking_itr in viewpoint.tracking_itr_splits:\n",
    "                    viewpoint.down_level()\n",
    "                render_pkg = render(\n",
    "                    viewpoint, self.gaussians, self.pipeline_params, self.background\n",
    "                )\n",
    "                image, depth, opacity = (\n",
    "                    render_pkg[\"render\"],\n",
    "                    render_pkg[\"depth\"],\n",
    "                    render_pkg[\"opacity\"],\n",
    "                )\n",
    "                pose_optimizer.zero_grad()\n",
    "                loss_tracking = get_loss_tracking(\n",
    "                    self.config, image, depth, opacity, viewpoint\n",
    "                )\n",
    "                loss_tracking.backward()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    pose_optimizer.step()\n",
    "                    converged = update_pose(viewpoint)\n",
    "\n",
    "                # if tracking_itr % 10 == 0:\n",
    "                #     self.q_main2vis.put(\n",
    "                #         gui_utils.GaussianPacket(\n",
    "                #             current_frame=viewpoint,\n",
    "                #             gtcolor=viewpoint.original_image,\n",
    "                #             gtdepth=viewpoint.depth\n",
    "                #             if not self.monocular\n",
    "                #             else np.zeros((viewpoint.image_height, viewpoint.image_width)),\n",
    "                #         )\n",
    "                #     )\n",
    "                if converged:\n",
    "                    break\n",
    "\n",
    "            self.median_depth = get_median_depth(depth, opacity)\n",
    "            return render_pkg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.config_utils import load_config\n",
    "from utils.dataset import load_dataset\n",
    "\n",
    "path_config = \"./configs/rgbd/tum/fr1_desk.yaml\"\n",
    "config = load_config(path_config)\n",
    "dataset = load_dataset(args=None, path=None, config=config)\n",
    "projection_matrix = getProjectionMatrix2(\n",
    "    znear=0.01,\n",
    "    zfar=100, \n",
    "    cx=dataset.cx, \n",
    "    cy=dataset.cy, \n",
    "    fx=dataset.fx, \n",
    "    fy=dataset.fy, \n",
    "    W=dataset.width, \n",
    "    H=dataset.height\n",
    ")\n",
    "viewpoint = Camera.init_from_dataset(dataset=dataset, idx=0, projection_matrix=projection_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "downsample_rgb = F.interpolate(viewpoint.original_image.unsqueeze(0), \n",
    "                                 scale_factor=0.5, \n",
    "                                 mode=\"bilinear\", \n",
    "                                 align_corners=False).squeeze()\n",
    "\n",
    "down_depth = F.interpolate(torch.from_numpy(viewpoint.depth).unsqueeze(0).unsqueeze(0), \n",
    "                                 scale_factor=0.5, \n",
    "                                 mode=\"bilinear\", \n",
    "                                 align_corners=False).squeeze()\n",
    "\n",
    "# plt.imshow(down_depth.cpu())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "visual_slam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
